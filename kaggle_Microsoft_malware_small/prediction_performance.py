import os
import sys

import pandas as pd
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix


def print_prediction_performance_from_files(predictions_csv, true_labels_csv):
    predictions = pd.read_csv(predictions_csv)
    true_labels = pd.read_csv(true_labels_csv)

    print_prediction_performance(
        predictions,
        true_labels,
        os.path.splitext(os.path.basename(predictions_csv))[0])


def print_prediction_performance(predictions, true_labels, title):
    actual_predictions = predictions.rename(columns={
        name: name[len('Prediction'):] if name.startswith(
            'Prediction') else name
        for name in predictions.columns})

    best_predictions = pd.DataFrame(columns=['Id', 'Class'])
    best_predictions['Id'] = actual_predictions['Id']
    best_predictions['Class'] = actual_predictions.drop('Id', axis=1).idxmax(axis=1)
    best_predictions['Class'] = pd.to_numeric(best_predictions['Class'])

    true_and_actual_predictions = best_predictions.merge(
        true_labels, left_on='Id', right_on='Id', suffixes=['_predicted', '_actual'])

    print(title)
    print()
    print(confusion_matrix(
        true_and_actual_predictions['Class_actual'],
        true_and_actual_predictions['Class_predicted']))
    print()
    print(classification_report(
        true_and_actual_predictions['Class_actual'],
        true_and_actual_predictions['Class_predicted']))


if __name__ == '__main__':
    if len(sys.argv) < 3:
        print(
            f'Usage: {sys.argv[0]} [path_to_predictions] [path_to_true_labels]',
            file=sys.stderr)
        exit(1)

    print_prediction_performance_from_files(sys.argv[1] ,sys.argv[2])
