import os
import sys

import pandas as pd
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix


def print_prediction_performance_from_files(predictions_csv, true_labels_csv):
    """Prints prediction metrics given a CSV containing predictions and a CSV
    containing true labels.

    Args:
        predictions_csv: Path to the CSV file containing predictions for each
        class. The CSV must contain the ``Id`` column denoting a file name
        corresponding to a sample under prediction, and ``Prediction[number]``
        columns containing probabilities for each class, where [number] is a
        class label (a positive integer).

        true_labels_csv: Path to the CSV file containing true labels. The CSV
        must contain a column named ``Id`` denoting a file name corresponding
        to a sample under prediction, and a column named ``Class`` denoting
        true class labels.
    """
    predictions = pd.read_csv(predictions_csv)
    true_labels = pd.read_csv(true_labels_csv)

    print_prediction_performance(
        predictions,
        true_labels,
        os.path.splitext(os.path.basename(predictions_csv))[0])


def print_prediction_performance(predictions, true_labels, title):
    actual_predictions = predictions.rename(columns={
        name: name[len('Prediction'):]
        if name.startswith('Prediction') else name
        for name in predictions.columns})

    best_predictions = pd.DataFrame(columns=['Id', 'Class'])
    best_predictions['Id'] = actual_predictions['Id']
    best_predictions['Class'] = actual_predictions.drop('Id', axis=1).idxmax(axis=1)
    best_predictions['Class'] = pd.to_numeric(best_predictions['Class'])

    true_and_actual_predictions = best_predictions.merge(
        true_labels, left_on='Id', right_on='Id', suffixes=['_predicted', '_actual'])

    print(title)
    print()
    print(confusion_matrix(
        true_and_actual_predictions['Class_actual'],
        true_and_actual_predictions['Class_predicted']))
    print()
    print(classification_report(
        true_and_actual_predictions['Class_actual'],
        true_and_actual_predictions['Class_predicted']))


if __name__ == '__main__':
    if len(sys.argv) < 3:
        print(
            f'Usage: {sys.argv[0]} [path_to_predictions] [path_to_true_labels]',
            file=sys.stderr)
        exit(1)

    print_prediction_performance_from_files(sys.argv[1] ,sys.argv[2])
